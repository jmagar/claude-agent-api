# openai-api Progress

## Goal

I want to make our API (apps/api) OpenAI compatible

## Status

Phase: design
Started: 2026-01-14
Completed: 2026-01-14

## Learnings

- OpenAI API has become the de facto standard for LLM inference, with many tools expecting this interface
- The core challenge is adapting OpenAI's message array format to Claude Agent SDK's single prompt format
- vLLM, LiteLLM, and LMStudio provide proven patterns for OpenAI compatibility layers
- Streaming format translation is achievable: OpenAI uses `delta` chunks vs our structured events
- Model name mapping (gpt-4 → sonnet) is straightforward via configuration
- Message history reconstruction from session state may be needed for multi-turn conversations
- The API already has SSE streaming infrastructure via sse-starlette, just needs format translation
- No additional dependencies needed - we're implementing server side, not client
- Translation layer should be isolated in new `/v1` route namespace to avoid polluting existing API
- Tool calling translation will be complex but can be deferred to Phase 2
- Token limits (max_tokens) vs turn limits (max_turns) requires approximation heuristic
- Authentication mapping (Authorization: Bearer → X-API-Key) is simple header translation
- Current architecture (protocol-based DI, Pydantic validation) supports adding translation layer cleanly
- Quality commands: ruff for linting, mypy/ty for type checking, pytest with 80% coverage target
- OpenAI streaming uses data: {json}\n format with [DONE] marker, similar to our SSE implementation
- Requirement scoping identified 7 user stories across 3 implementation phases
- Type safety is critical: ZERO tolerance for `Any` types, must use TypedDict for all JSON structures
- Message concatenation strategy: system messages become system_prompt, user/assistant joined with role prefixes
- Parameter incompatibilities require careful handling: max_tokens vs max_turns needs heuristic conversion
- Session state management differs: OpenAI is stateless, Claude Agent SDK is stateful (session_id continuity)
- Error format translation must maintain security while matching OpenAI schema (type, message, code)
- Streaming adapter needs to be stateful to generate proper delta chunks from content block events
- Tool calling deferred to Phase 2 due to complex schema translation requirements
- Risk analysis reveals 10 key risks with mitigation strategies, highest impact: message history and tool format
- Success criteria defined: OpenAI Python client works with only base_url change, 80% test coverage, <5ms translation latency
- Phase 1 MVP scope: basic chat completions (streaming + non-streaming), auth, models endpoint
- Phase 2 scope: tool calling support
- Phase 3 scope: advanced parameters and performance optimization

## Design Phase Learnings

- Architecture isolated in `/v1` namespace with zero impact on existing `/api/v1` endpoints
- Translation layer follows existing protocol-based dependency injection pattern exactly
- Five key components: RequestTranslator, ResponseTranslator, StreamingAdapter, ModelMapper, ErrorTranslator
- BearerAuthMiddleware extracts `Authorization: Bearer` token and maps to `X-API-Key` for existing auth
- Message concatenation uses role prefixes: `USER: content\n\nASSISTANT: content\n\n`
- Token-to-turn conversion uses heuristic: `max_turns = max(1, max_tokens // 500)`
- Content extraction concatenates all text blocks, ignores thinking/tool_use blocks
- Stop reason mapping: completed→stop, max_turns_reached→length, interrupted→stop, error→error
- Streaming adapter maintains state to generate consistent completion IDs across chunks
- Model mapping hardcoded dict (injectable): gpt-4→sonnet, gpt-3.5-turbo→haiku, gpt-4o→opus
- All JSON structures use TypedDict (zero `Any` types), Pydantic only for request validation
- Error translator maps HTTP status codes to OpenAI error types (401→authentication_error, 400→invalid_request_error, 429→rate_limit_exceeded, 500→api_error)
- Exception handlers check route prefix to apply OpenAI error format only for `/v1/*` paths
- SSE format difference: Native has `event` field, OpenAI uses `data:` prefix only with `[DONE]` marker
- Performance targets: <5ms request translation, <10ms response translation, <50ms first chunk
- Test strategy: ≥90% coverage for translators, ≥80% for routes, contract tests with real OpenAI client
- Implementation phases: Core translation (day 1-2), non-streaming (day 2-3), streaming (day 3-4), models endpoint (day 4), contract tests (day 4-5), docs (day 5)
- Edge cases handled: empty messages, only system messages, unknown models, missing usage data, mixed content blocks
- Existing patterns preserved: structlog logging, TypedDict for JSON, Pydantic for validation, protocol-based DI
- Middleware order critical: BearerAuthMiddleware runs before ApiKeyAuthMiddleware to add X-API-Key header
- Configuration extends Settings class with openai_enabled flag and openai_model_mapping dict
- File structure creates new `routes/openai/`, `schemas/openai/`, `services/openai/` directories
- No new dependencies required - translation is pure Python with existing FastAPI/Pydantic infrastructure

## Task Planning Learnings

- Task breakdown: 42 tasks across 4 phases (POC, Refactoring, Testing, Quality Gates)
- POC phase (15 tasks): Focus on validating translation layer works end-to-end before adding tests
- Quality checkpoints inserted every 2-3 tasks to catch type/lint issues early (6 checkpoint tasks total)
- Dependency order critical: TypedDict schemas → services (ModelMapper, translators) → middleware → routes → registration
- POC validation uses manual OpenAI client test script before writing formal test suite
- Refactoring phase (5 tasks): Extract DI helpers, improve error handling, add logging, extract helpers
- Testing phase (9 tasks): Unit tests for services (≥90% coverage), integration for routes (≥80%), contract tests with real OpenAI client
- Quality gates (4 tasks): Full CI simulation, documentation, PR creation with CI verification
- Task verification strategy: Each task has explicit command to prove completion (type check, manual curl, pytest)
- Commit message convention: feat/test/refactor/docs/chore with (openai) scope
- Risk mitigation: Early quality checkpoints prevent accumulation of type errors and lint violations
- POC shortcuts documented: Hardcoded mappings, basic concatenation, simple heuristics (can enhance in Phase 2/3)
- Test progression: Unit (services) → Integration (routes) → Contract (OpenAI client) ensures layer-by-layer validation
- Final phase includes documentation updates (README, CLAUDE.md) and PR with CI monitoring
- Success criteria tied to measurable outcomes: coverage percentages, type check passes, OpenAI client works
- Estimated effort: 15 POC tasks (2-3 days), 5 refactoring tasks (0.5 day), 9 test tasks (1-2 days), 4 quality tasks (0.5 day) = 4-6 days total

## TDD Task Planning Learnings (REVISED)

- **CRITICAL SHIFT**: Tasks restructured to follow STRICT TDD - test FIRST, implementation SECOND, no exceptions
- **Task count updated**: 48 tasks (from 42) - more granular to ensure each RED-GREEN-REFACTOR cycle is explicit
- **RED-GREEN-REFACTOR workflow**: Every implementation task shows failing test → minimal code → passing test → optional refactor
- **Phase 1 restructured**: Core Translation Services (TDD) - 16 tasks building services test-first, one component at a time
- **Phase 2 restructured**: Middleware & Routes (TDD) - 10 tasks with integration tests driving implementation
- **Phase 3**: Contract Testing & Refinement - 6 tasks validating OpenAI client compatibility and refactoring
- **Phase 4**: Quality Gates & Documentation - 5 tasks for coverage, docs, CI
- **Test coverage enforced**: ≥90% for services (unit tests), ≥80% for routes (integration tests)
- **No POC-first approach**: Eliminated "make it work then test" pattern - tests always come first
- **Quality checkpoints remain**: 6 checkpoints every 2-3 tasks to catch type/lint issues early
- **Verification commands explicit**: Each task includes exact pytest command to run for RED/GREEN validation
- **Schema definitions exempt from TDD**: TypedDict/Pydantic schemas created without tests (type definitions, not behavior)
- **Integration tests drive routes**: HTTP layer implemented only after writing failing integration test
- **Contract tests validate compliance**: Real OpenAI Python client used to verify API compatibility
- **Refactoring is safe**: Tests written first means refactoring can happen with confidence (tests stay green)
- **Task dependencies clear**: Must complete schemas → ModelMapper → RequestTranslator → ResponseTranslator → middleware → routes
- **Edge cases identified early**: Test-first approach forces thinking about edge cases (empty messages, unknown models, etc.) during RED phase
- **Estimated effort unchanged**: Still 4-6 days total, but higher code quality from day 1 due to TDD discipline

## SDK Research Learnings

- **Temperature NOT SUPPORTED**: Claude Agent SDK does NOT have temperature, top_p, or max_tokens parameters
- **ClaudeAgentOptions analyzed**: Reviewed `.venv/lib/python3.12/site-packages/claude_agent_sdk/types.py` (lines 616-681)
- **No sampling controls**: No temperature, top_p, stop sequences in SDK at all (grepped entire package)
- **extra_args exists**: `extra_args: dict[str, str | None]` for "arbitrary CLI flags" but no CLI support for sampling
- **max_turns only**: SDK uses `max_turns` (conversation turn limit), NOT `max_tokens` (output token limit)
- **Decision**: Accept unsupported OpenAI parameters (temperature, top_p, max_tokens, stop) and log structured warnings
- **Max tokens handling**: Ignore max_tokens completely - do NOT map to max_turns (incompatible semantics)
- **Session management**: SDK uses `resume`, `continue_conversation`, `fork_session` - NOT message history arrays
- **Message history strategy**: Concatenate ALL messages for new sessions, use ONLY last message for resumed sessions
- **Session mapping required**: Redis cache maps OpenAI conversation hash → SDK session_id (enables stateless API)
- **Native SSE events**: Pydantic models (InitEvent, MessageEvent, etc.), NOT plain dicts - use structured parsing
- **stop_reason enum**: "completed", "max_turns_reached", "interrupted", "error" - map to OpenAI finish_reason
- **Type checker**: Use ty (Astral's fast checker), NOT mypy - better Any detection, aligns with uv/ruff
- **Type strategy**: Pydantic for requests (validation), TypedDict for responses (zero overhead)
- **TypedDict location**: Extend existing `apps/api/types.py` (don't create new schemas/openai/types.py)
- **Auth strategy**: BearerAuthMiddleware extracts Bearer token → sets X-API-Key header (bridge pattern)
- **Middleware order**: Register ApiKeyAuthMiddleware BEFORE BearerAuthMiddleware (executes in REVERSE order)
- **Error mapping**: Read apps/api/exceptions.py, map HTTP status codes to OpenAI error types
- **Model config**: Dict in Settings class with env var support: `OPENAI_MODEL_MAPPING='{"gpt-4":"sonnet"}'`
- **Protocol location**: Create `apps/api/protocols/openai.py` for translation layer abstractions
- **Tool_use serialization**: Serialize tool_use blocks as text when no text content (for visibility)
- **Unknown stop reasons**: Default to "stop" with warning log (graceful degradation)
- **Benchmarks deferred**: Phase 3 only - focus Phase 1 on correctness
- **Critical issues identified**: 26 critical issues across 4 spec docs - MUST fix before implementation
- **Decisions documented**: Created `specs/openai-api/decisions.md` with all architectural decisions and rationale

## Completed Tasks

- [x] 1.1 Create package structure (no tests needed) - 1ce8745
- [x] 1.2 Create TypedDict schemas (no tests needed for type definitions) - 5bf4eaa
- [x] 1.3 Create Pydantic request schemas (no tests needed for schema definitions) - f24484b
- [x] 1.4 RED - Write ModelMapper tests - 9505e47
- [x] 1.5 RED - Write RequestTranslator test for basic user message - e694a3d
- [x] 1.6 RED - Write RequestTranslator test for system message extraction - 194d4d1
- [x] 1.7 RED - Write RequestTranslator test for multi-turn conversation - 8a9a85c
- [x] 1.8 RED - Write RequestTranslator test for max_tokens handling - 6d05dec
- [x] 1.9 RED - Write RequestTranslator test for parameter handling - 7927e49
- [x] 1.10 [VERIFY] Quality checkpoint 1 - f550bc1
- [x] 1.11 RED - Write ResponseTranslator test for basic response - 72ed00b
- [x] 1.12 RED - Write ResponseTranslator test for usage mapping - 86a04d4
- [x] 1.13 RED - Write ResponseTranslator test for stop reason mapping - d3b8b48

## Current Task

Awaiting next task

## Learnings

- TDD RED-GREEN-REFACTOR workflow validated: Write failing test → Implement → Verify passing
- ModelMapper requires bidirectional mapping (OpenAI ↔ Claude) for reverse lookups
- OpenAIModelInfo TypedDict provides proper type safety for model list responses
- Fixture approach in pytest enables reusable test configuration
- ValueError exceptions provide clear error messages for unknown models
- Static metadata (created timestamp, owned_by) hardcoded for simplicity in POC phase
- RequestTranslator concatenates messages with role prefixes: "USER: content\n\n"
- QueryRequest from existing schema has many optional fields, only prompt and model are required
- Message concatenation is simple string joining, no complex parsing needed at this stage
- System messages extracted to system_prompt field, separated from conversation prompt
- Helper function _separate_system_messages() encapsulates system message extraction logic
- Multiple system messages combined with "\n\n" separator for single system_prompt
- Refactoring after GREEN phase maintains test coverage while improving code structure
- Multi-turn conversation test passes immediately - implementation already correct for multiple messages
- translate() method uses loop to handle arbitrary number of user/assistant messages
- Test validates existing behavior even when implementation is already correct (TDD verification pattern)
- max_tokens parameter accepted for OpenAI compatibility but NOT mapped to max_turns (incompatible semantics)
- structlog warning logs captured with pytest capsys fixture (stdout/stderr), not caplog
- _log_unsupported_parameter() helper extracted for reusable unsupported parameter warnings
- SDK limitation: max_tokens (output token limit) has no equivalent in Claude Agent SDK (max_turns is conversation turn limit)
- Clear inline comments document design decisions (why max_turns not set when max_tokens present)
- temperature, top_p, and stop parameters unsupported by Claude Agent SDK (sampling controls not available)
- User parameter SUPPORTED by SDK - successfully mapped to QueryRequest.user field
- stop field added to ChatCompletionRequest schema (list[str] | str | None to match OpenAI spec)
- Four new parameter tests added: temperature warning, top_p warning, stop warning, user mapping
- All unsupported parameters log structured warnings via _log_unsupported_parameter() helper
- Quality checkpoint passed with 3 lint errors fixed (import sorting, __all__ sorting, dict.keys() simplification)
- Auto-fix with ruff handled import/export sorting, manual fix for dict iteration pattern (use dict directly, not .keys())
- All 16 tests passing after lint fixes, demonstrating proper TDD green-refactor workflow
- ResponseTranslator implements basic translation: SingleQueryResponse → OpenAIChatCompletion
- Content extraction filters text-only blocks, concatenates with space separator
- Completion ID generated with f"chatcmpl-{uuid.uuid4()}" format (OpenAI compatible)
- Usage mapping: input_tokens → prompt_tokens, output_tokens → completion_tokens, total calculated
- Created timestamp uses int(time.time()) for Unix epoch seconds
- Minimal implementation passes all test assertions: content, model, object type, ID format, timestamp validity
- Test added for usage mapping validation even though implementation already existed (ensures test coverage)
- Test passes immediately (GREEN) because implementation was already present from previous task
- Usage field properly handles missing usage data with default zeros (defensive programming)
- Stop reason mapping implemented with _map_stop_reason() helper method
- Mapping: completed→stop, max_turns_reached→length, interrupted→stop, error→error, None→stop
- Helper method enables centralized mapping logic for easy maintenance and testing
- All four stop_reason tests pass after implementing mapping logic (TDD GREEN phase successful)

## Next

Task 1.14: RED - Write ResponseTranslator test for multiple content blocks
