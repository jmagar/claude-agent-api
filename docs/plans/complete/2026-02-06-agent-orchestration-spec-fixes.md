# Agent Orchestration Spec Fixes Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Fix all 18 identified issues in agent orchestration spec artifacts to align with actual Mem0 OSS implementation and complete missing sections.

**Architecture:** Update spec.md to reflect Mem0 architecture, integrate spec-additions.md sections, fix inconsistencies across plan.md and AGENT-README.md, add implementation status tracking.

**Tech Stack:** Markdown documentation, no code changes (spec artifacts only)

---

## Critical Issues (Blockers)

### Task 1: Update Phase 1 Memory System to Reflect Mem0 Architecture

**Files:**
- Modify: `specs/agent-orchestration/spec.md:101-176` (Memory System section)
- Reference: `CLAUDE.md:## Mem0 OSS Integration` (lines in root)

**Step 1: Replace custom memory implementation with Mem0 description**

**Current (spec.md lines 101-176):**
```markdown
**Our implementation:**
- **Leverage existing infrastructure:**
  - TEI (cli-firecrawl) for embedding generation
  - Qdrant (cli-firecrawl) for vector storage and semantic search
  - Memory bank (`~/memory/bank/`) for structured data
- PostgreSQL for additional memory metadata
- Redis for active session cache
- Memory types: facts, preferences, relationships, workflows
```

**Replace with:**
```markdown
**Our implementation:**
- **Mem0 OSS Integration** - Pre-built memory system with graph-enhanced capabilities
  - LLM: Gemini 3 Flash Preview (cli-api.tootie.tv) for memory extraction
  - Embeddings: Qwen/Qwen3-Embedding-0.6B (1024-dim) via TEI at 100.74.16.82:52000
  - Vector Store: Qdrant at localhost:53333 for semantic search
  - Graph Store: Neo4j at localhost:54687 for entity/relationship storage
  - History: SQLite at ~/.mem0/history.db for metadata
- Memory bank (`~/memory/bank/`) for structured snapshots (existing homelab)
- Multi-tenant isolation via user_id/agent_id scoping
```

**Step 2: Update Memory class definition**

**Current (lines 119-130):**
```python
class Memory:
    id: str
    content: str
    memory_type: Literal["fact", "preference", "relationship", "workflow"]
    embedding: list[float]  # Generated via TEI, 384-dim (BAAI/bge-small-en-v1.5)
    embedding_model: str  # e.g., "BAAI/bge-small-en-v1.5"
    created_at: datetime
    last_accessed: datetime
    access_count: int
    source: Literal["user", "inferred", "imported"]
```

**Replace with:**
```python
class Memory:
    id: str
    memory: str  # Mem0 uses "memory" field, not "content"
    user_id: str  # For multi-tenant isolation (maps to api_key)
    agent_id: str = "main"
    metadata: dict[str, str | int | float | bool]
    created_at: datetime
    updated_at: datetime
    # Note: Embeddings stored in Qdrant, not exposed via Memory object
    # Note: Graph relationships stored in Neo4j, queried separately
```

**Step 3: Update Embedding Storage section**

**Current (lines 132-139):**
```markdown
**Embedding Storage:**
- **Dimensionality**: 384 dimensions (BAAI/bge-small-en-v1.5 default)
- **Storage Strategy**: Embeddings stored ONLY in Qdrant vector DB
  - PostgreSQL stores memory metadata + reference to Qdrant point ID
  - Reduces DB size: 384 floats √ó 4 bytes = 1.5KB per embedding
  - 10K memories = ~15MB in Qdrant vs ~15MB + metadata in PostgreSQL
```

**Replace with:**
```markdown
**Embedding Storage (Mem0 Architecture):**
- **Dimensionality**: 1024 dimensions (Qwen/Qwen3-Embedding-0.6B)
- **Storage Strategy**: Dual storage for vector + graph
  - Qdrant: Vector embeddings with payload metadata (user_id, agent_id filtering)
  - Neo4j: Entity/relationship graph for contextual retrieval
  - SQLite: History and job metadata at ~/.mem0/history.db
  - Size: 1024 floats √ó 4 bytes = 4KB per embedding
  - 10K memories = ~40MB in Qdrant (vs 15MB with 384-dim)
```

**Step 4: Add Mem0 Graph Memory section**

**Insert after Embedding Storage section (after line 139):**

```markdown
**Graph Memory Features:**

Mem0 provides automatic entity and relationship extraction for contextual memory:

**How it works:**
1. User conversation ‚Üí Mem0 LLM extracts entities (people, places, concepts)
2. Entities stored in Neo4j with relationships (WORKS_AT, LIVES_IN, USES, etc.)
3. Vector search returns relevant memories + related graph context
4. Graph context enhances retrieval with relationship understanding

**Example:**
```
Query: "What programming languages does the user like?"
Vector match: "User likes Python" (score: 0.92)
Graph context:
  - User USES Python
  - Python RELATED_TO FastAPI
  - User USES FastAPI
  - FastAPI RELATED_TO API_development
```

**Performance Trade-off:**
- Graph operations add ~100-200ms latency per request
- Disable with `enable_graph=False` for high-frequency operations
- Enable for queries requiring relationship understanding

**Multi-Tenant Isolation:**
- Vector store: Payload filters on `user_id` and `agent_id`
- Graph store: Entities tagged with ownership metadata
- No cross-tenant contamination (API keys cannot access other tenants' data)
```

**Step 5: Update Memory Security section**

**Current (lines 141-176):**
```markdown
1. **Encryption at Rest:**
   - PostgreSQL: Use disk-level encryption (LUKS/dm-crypt) or pg_tde extension for TDE
   - Qdrant: Disk encryption via LUKS/dm-crypt at OS level
```

**Replace with:**
```markdown
1. **Encryption at Rest:**
   - Qdrant: Disk encryption via LUKS/dm-crypt at OS level
   - Neo4j: Built-in encryption support or disk-level encryption
   - SQLite: File-level encryption (sqlcipher) or disk encryption
```

**Step 6: Update Data Retention section**

**Current (lines 148-155):**
```markdown
2. **Data Retention & Garbage Collection:**
   - Configurable TTL per memory type:
     - Facts: 365 days (long-term knowledge)
     - Preferences: 180 days (may change over time)
     - Relationships: 730 days (stable over years)
     - Workflows: 90 days (task-specific, short-lived)
   - Background GC job runs daily, deletes expired memories from PostgreSQL + Qdrant
   - Manual deletion cascade: PostgreSQL DELETE ‚Üí trigger Qdrant point deletion
```

**Replace with:**
```markdown
2. **Data Retention & Garbage Collection:**
   - Mem0 handles storage internally, no built-in TTL
   - Custom GC implementation via API:
     - Query all memories: `memory.get_all(user_id=api_key)`
     - Filter by `created_at` timestamp in metadata
     - Delete expired: `memory.delete(memory_id=id, user_id=api_key)`
   - Background job runs daily, deletes based on retention policy
   - Cascade deletion: Mem0 removes from Qdrant + Neo4j + SQLite automatically
```

**Step 7: Update Access Control section**

**Current (lines 157-161):**
```markdown
3. **Access Control:**
   - All memory reads/writes scoped to authenticated API key
   - Redis/PostgreSQL memory keys: `memory:{api_key}:{memory_id}`
   - Qdrant collections use API key as namespace filter (multi-tenant isolation)
   - Cannot read/write other tenants' memories
```

**Replace with:**
```markdown
3. **Access Control:**
   - All memory reads/writes scoped to `user_id` (maps to API key)
   - Mem0 API calls include `user_id` parameter for tenant isolation
   - Example: `memory.search(query="...", user_id=api_key, agent_id="main")`
   - Qdrant filters applied at query time using payload filters
   - Neo4j queries filter by entity ownership metadata
   - Cannot read/write other tenants' memories (enforced by Mem0)
```

**Step 8: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "fix: update Phase 1 memory system to reflect Mem0 OSS architecture

- Replace custom memory implementation with Mem0 integration details
- Update embedding dimensions from 384 to 1024 (Qwen model)
- Add graph memory features section (Neo4j integration)
- Update storage architecture (Qdrant + Neo4j + SQLite)
- Fix multi-tenant isolation description (user_id scoping)
- Update data retention and access control sections

Addresses Issue #1 from spec analysis"
```

---

### Task 2: Integrate spec-additions.md Sections into spec.md

**Files:**
- Modify: `specs/agent-orchestration/spec.md` (multiple insertion points)
- Reference: `specs/agent-orchestration/spec-additions.md`

**Step 1: Add SessionSearchConfig Privacy Extensions**

**Insert after spec.md line 473 (end of SessionSearchConfig definition):**

```markdown
    exclude_paths: list[str] = []  # Paths to exclude from indexing
    redact_patterns: list[str] = []  # Regex patterns for redaction
    require_consent: bool = True  # Require explicit consent for session indexing
    retention_days: int | None = None  # None = infinite retention

**Privacy & Sanitization:**

Session data may contain sensitive information (API keys, tokens, passwords, PII). The `sanitize_session_text()` routine runs **before** embedding generation and vector storage to protect user privacy:

[... copy full section from spec-additions.md lines 22-97 ...]
```

**Step 2: Add Resilience & Failure Modes section**

**Insert after Architecture diagram (after line 532), before OpenClaw Feature Parity section:**

```markdown
## Resilience & Failure Modes

[... copy full section from spec-additions.md lines 99-284 ...]
```

**Step 3: Add Authentication & Authorization section**

**Insert after Resilience & Failure Modes section:**

```markdown
## Authentication & Authorization

[... copy full section from spec-additions.md lines 285-473 ...]
```

**Step 4: Update HeartbeatConfig timezone field**

**Modify spec.md line 279:**

**Current:**
```python
    timezone: str = "America/New_York"
```

**Replace with:**
```python
    timezone: str | None = None  # None = system timezone, or IANA timezone string
```

**Step 5: Add timezone handling section after HeartbeatConfig**

**Insert after HeartbeatConfig class definition:**

```markdown
**Timezone Handling:**

[... copy full section from spec-additions.md lines 502-554 ...]
```

**Step 6: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "feat: integrate missing spec sections from spec-additions.md

- Add SessionSearchConfig privacy extensions and sanitization
- Add Resilience & Failure Modes section (dependency failures, timeouts, health checks)
- Add Authentication & Authorization section (API keys, scoping, permissions, rate limits)
- Update HeartbeatConfig timezone to support system timezone detection
- Add timezone handling logic and validation

Addresses Issue #2 from spec analysis"
```

---

### Task 3: Fix Memory API Endpoint Inconsistencies

**Files:**
- Modify: `specs/agent-orchestration/spec.md:729-736`
- Reference: `docs/plans/2026-02-03-mem0-oss-integration-part2.md:77-176`

**Step 1: Update Memory Endpoints section**

**Current (spec.md lines 729-736):**
```http
### Memory Endpoints (New)

GET    /api/v1/memory                    # List memories
POST   /api/v1/memory                    # Create memory
GET    /api/v1/memory/search             # Search memories (semantic)
DELETE /api/v1/memory/{id}               # Delete memory
```

**Replace with:**
```http
### Memory Endpoints (New)

GET    /api/v1/memories                      # List all memories for user
POST   /api/v1/memories                      # Add memory (from conversation)
POST   /api/v1/memories/search               # Search memories (semantic)
DELETE /api/v1/memories/{memory_id}          # Delete single memory
DELETE /api/v1/memories                      # Delete ALL memories for user
GET    /api/v1/memories/{memory_id}          # Get memory by ID (optional)
```

**Step 2: Add detailed endpoint documentation**

**Insert after endpoint list:**

```markdown
**Endpoint Details:**

**POST /api/v1/memories**
```json
{
  "messages": "User prefers technical explanations",
  "metadata": {"category": "preferences"}
}
```
Response: `201 Created`
```json
{
  "memories": [
    {"id": "mem_123", "memory": "User prefers technical explanations"}
  ],
  "message": "Memory added successfully"
}
```

**POST /api/v1/memories/search**
```json
{
  "query": "What are user preferences?",
  "limit": 10
}
```
Response: `200 OK`
```json
{
  "results": [
    {
      "id": "mem_123",
      "memory": "User prefers technical explanations",
      "score": 0.95,
      "metadata": {"category": "preferences"}
    }
  ],
  "count": 1
}
```

**GET /api/v1/memories**
Response: `200 OK`
```json
{
  "memories": [
    {"id": "mem_123", "memory": "User prefers technical explanations"},
    {"id": "mem_456", "memory": "User likes Python"}
  ],
  "count": 2
}
```

**DELETE /api/v1/memories/{memory_id}**
Response: `200 OK`
```json
{
  "deleted": true,
  "memory_id": "mem_123",
  "message": "Memory deleted successfully"
}
```

**DELETE /api/v1/memories** (Delete all)
Response: `200 OK`
```json
{
  "deleted": true,
  "count": 42,
  "message": "All memories deleted successfully"
}
```
```

**Step 3: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "fix: correct memory API endpoints to match implementation

- Change singular '/memory' to plural '/memories'
- Add POST /api/v1/memories/search (semantic search)
- Add DELETE /api/v1/memories (delete all for user)
- Add detailed request/response examples
- Align with actual Mem0 integration routes

Addresses Issue #3 from spec analysis"
```

---

### Task 4: Update Embedding Dimensions Throughout Spec

**Files:**
- Modify: `specs/agent-orchestration/spec.md` (multiple occurrences)

**Step 1: Search and replace all 384-dim references**

Find all occurrences:
```bash
grep -n "384" specs/agent-orchestration/spec.md
```

Expected matches:
- Line 123: `embedding: list[float]  # Generated via TEI, 384-dim`
- Line 133: `**Dimensionality**: 384 dimensions`
- Line 136: `Reduces DB size: 384 floats √ó 4 bytes = 1.5KB`

**Step 2: Update each occurrence**

Already done in Task 1, Step 3 (Embedding Storage section).

**Step 3: Search for BAAI model references**

Find all occurrences:
```bash
grep -n "BAAI/bge-small-en-v1.5" specs/agent-orchestration/spec.md
```

**Replace with:**
```
Qwen/Qwen3-Embedding-0.6B
```

**Step 4: Verify no remaining 384-dim references**

```bash
grep -n "384" specs/agent-orchestration/spec.md
```

Expected: No matches (all replaced with 1024)

**Step 5: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "fix: update embedding dimensions from 384 to 1024 throughout

- Replace BAAI/bge-small-en-v1.5 (384-dim) with Qwen/Qwen3-Embedding-0.6B (1024-dim)
- Update storage size calculations (1.5KB ‚Üí 4KB per embedding)
- Update collection size estimates (15MB ‚Üí 40MB for 10K memories)

Addresses Issue #4 from spec analysis"
```

---

### Task 5: Document Mem0 Features in spec.md

**Files:**
- Modify: `specs/agent-orchestration/spec.md` (Phase 1 section)

**Step 1: Add "Mem0 vs Custom Implementation" comparison**

**Insert after Memory Security section (after line 176):**

```markdown
### Mem0 OSS vs Custom Implementation

**Why we chose Mem0:**

| Aspect | Custom Implementation | Mem0 OSS |
|--------|----------------------|----------|
| Vector search | Manual TEI + Qdrant integration | Built-in with optimized queries |
| Graph memory | Would need custom Neo4j code | Automatic entity/relationship extraction |
| Multi-tenant | Manual filtering logic | Built-in user_id/agent_id scoping |
| LLM extraction | Custom prompt engineering | Pre-trained extraction pipeline |
| Maintenance | Our responsibility | Community-maintained |
| Time to implement | 2-3 weeks | 2-3 days |

**Mem0 provides:**
- Production-ready memory management
- Graph-enhanced retrieval (entities + relationships)
- Automatic LLM-based extraction
- Multi-tenant isolation built-in
- Active community and updates

**Trade-offs:**
- Less control over internal behavior
- Dependency on external library
- Graph operations add latency (~100-200ms)

**Decision:** Use Mem0 for Phase 1. Custom implementation only if Mem0 proves insufficient.
```

**Step 2: Add Mem0 Configuration Example**

**Insert after comparison table:**

```markdown
### Mem0 Configuration

**Complete configuration (from CLAUDE.md):**

```python
from mem0 import Memory
import os

config = {
    # LLM Provider (for memory extraction)
    "llm": {
        "provider": "openai",
        "config": {
            "base_url": "https://cli-api.tootie.tv/v1",
            "model": "gemini-3-flash-preview",
            "api_key": os.environ.get("LLM_API_KEY", "")
        }
    },

    # Embedder (TEI on remote host)
    "embedder": {
        "provider": "openai",  # TEI exposes OpenAI-compatible API
        "config": {
            "model": "text-embedding-3-small",  # Dummy name (TEI ignores)
            "openai_base_url": "http://100.74.16.82:52000/v1",
            "embedding_dims": 1024,
            "api_key": "not-needed"
        }
    },

    # Vector Store (Qdrant)
    "vector_store": {
        "provider": "qdrant",
        "config": {
            "host": "localhost",
            "port": 53333,
            "collection_name": "mem0_memories",
            "embedding_model_dims": 1024,
            "distance": "cosine",
            "on_disk": True
        }
    },

    # Graph Store (Neo4j)
    "graph_store": {
        "provider": "neo4j",
        "config": {
            "url": "bolt://localhost:54687",
            "username": "neo4j",
            "password": "neo4jpassword",
            "database": "neo4j"
        }
    },

    "version": "v1.1"
}

memory = Memory.from_config(config)
```
```

**Step 3: Add performance tuning section**

**Insert after configuration:**

```markdown
### Mem0 Performance Tuning

**Graph memory toggle:**
```python
# Disable graph for high-frequency operations (faster, no relationships)
results = memory.search(
    query="user preferences",
    user_id=api_key,
    enable_graph=False  # ~50ms instead of ~200ms
)

# Enable graph for contextual queries (slower, with relationships)
results = memory.search(
    query="what does the user work on?",
    user_id=api_key,
    enable_graph=True  # Returns entities + relationships
)
```

**When to disable graph:**
- Heartbeat checks (run every 30min, latency matters)
- Real-time chat responses (user waiting)
- Background batch jobs (throughput > latency)

**When to enable graph:**
- Complex queries requiring relationships
- Initial conversation context retrieval
- Explicit user questions about past interactions
```

**Step 4: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "docs: add Mem0 features and configuration documentation

- Add comparison table (Mem0 vs custom implementation)
- Add complete Mem0 configuration example
- Add performance tuning guide (enable_graph toggle)
- Document when to use graph memory vs vector-only

Addresses Issue #6 from spec analysis"
```

---

## High Priority Issues

### Task 6: Clarify synapse-mcp Integration Approach

**Files:**
- Modify: `specs/agent-orchestration/plan.md:1244-1358`
- Modify: `specs/agent-orchestration/spec.md:386-399`

**Step 1: Add decision section to spec.md**

**Insert after synapse-mcp section (after line 399):**

```markdown
**Integration Approach:**

synapse-mcp can be used in two modes:

1. **stdio mode** (default, recommended):
   - MCP server runs as subprocess
   - Managed by Claude Agent SDK
   - Auto-starts on first tool call
   - Configuration in `.mcp-server-config.json`

2. **HTTP mode** (alternative, for remote infrastructure):
   - HTTP server runs independently
   - Client makes REST calls
   - Requires separate server deployment
   - Useful for multi-user environments

**Our choice: stdio mode**
- Simpler deployment (no separate server)
- SDK handles lifecycle automatically
- Localhost-only (no network exposure)
- Sufficient for single-user personal assistant

**HTTP mode use case:**
- Multiple API instances sharing infrastructure access
- Remote infrastructure management
- Rate limiting and quotas needed
```

**Step 2: Update plan.md to match stdio approach**

**Modify plan.md lines 1244-1358 (SynapseClient section):**

**Current:**
```python
class SynapseClient:
    async def flux(...) -> dict:
        async with AsyncClient() as client:
            response = await client.post(f"{self.base_url}/mcp", ...)
```

**Replace with:**
```python
# synapse-mcp integrated via MCP config, not HTTP client
# SDK handles all communication via stdio

# .mcp-server-config.json:
{
  "mcpServers": {
    "synapse": {
      "type": "stdio",
      "command": "node",
      "args": ["../synapse-mcp/dist/index.js"],
      "env": {
        "SYNAPSE_CONFIG_FILE": "../synapse-mcp/synapse.config.json"
      }
    }
  }
}

# Usage in code - no custom client needed:
response = await sdk_client.query(
    "Check container status on all hosts",
    mcp_servers=None  # Uses application-tier config
)
```

**Step 3: Commit changes**

```bash
git add specs/agent-orchestration/spec.md specs/agent-orchestration/plan.md
git commit -m "fix: clarify synapse-mcp integration uses stdio mode, not HTTP

- Add integration approach decision section
- Document stdio vs HTTP mode trade-offs
- Update plan.md to remove HTTP client code
- Specify SDK handles all MCP communication

Addresses Issue #8 from spec analysis"
```

---

### Task 7: Add Device Management Decision Matrix

**Files:**
- Modify: `specs/agent-orchestration/spec.md` (Device Management section)

**Step 1: Add decision matrix after Device Management section**

**Insert after spec.md line 373 (end of Device Management):**

```markdown
### Device Management: Memory Bank vs synapse-mcp

**Decision matrix for tool selection:**

| Use Case | Tool | Reason |
|----------|------|--------|
| **Static device inventory** | Memory bank (`~/memory/bank/ssh/`) | Pre-indexed, fast reads, historical snapshots |
| **Device capabilities** | Memory bank | OS type, Docker availability, systemd, SSH config |
| **Real-time container status** | synapse-mcp Flux | Live data, start/stop/restart operations |
| **Container logs with grep** | synapse-mcp Flux | Streaming logs, regex filtering |
| **Compose project management** | synapse-mcp Flux | Up/down/restart, multi-service coordination |
| **ZFS pool health** | synapse-mcp Scout | Live SMART data, scrub status, capacity |
| **File transfers** | synapse-mcp Scout | Secure copy with chunking |
| **System resource monitoring** | synapse-mcp Flux (host) | CPU/RAM/disk in real-time |
| **Command execution** | synapse-mcp Scout (exec) | Allowlisted commands with timeout |
| **Historical trends** | Memory bank | Compare snapshots over time |

**Rule of thumb:**
- **Memory bank**: "What devices exist and what are their capabilities?"
- **synapse-mcp**: "What is the current state and can you change it?"

**Example workflow:**
1. Query memory bank for device list ‚Üí ["clawd", "shart", "squirts"]
2. Use synapse-mcp Flux to check container status on each host
3. Use synapse-mcp Scout to read logs if issues found
4. Write updated status to memory bank for historical tracking
```

**Step 2: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "docs: add decision matrix for memory bank vs synapse-mcp usage

- Document when to use memory bank (static inventory, historical)
- Document when to use synapse-mcp (real-time, operations)
- Add example workflow combining both tools
- Clarify complementary nature of both systems

Addresses Issue #9 from spec analysis"
```

---

### Task 8: Specify Persona Configuration Storage

**Files:**
- Modify: `specs/agent-orchestration/spec.md:796-801`
- Modify: `specs/agent-orchestration/AGENT-README.md:645-659`
- Modify: `specs/agent-orchestration/plan.md:1599-1617`

**Step 1: Add Persona Storage section to spec.md**

**Replace spec.md lines 796-801:**

**Current:**
```http
### Persona Endpoints (New)

GET    /api/v1/persona                   # Get persona config
PUT    /api/v1/persona                   # Update persona config
```

**Replace with:**
```http
### Persona Endpoints (New)

GET    /api/v1/persona                   # Get persona config
PUT    /api/v1/persona                   # Update persona config

**Storage Strategy:**

Primary: PostgreSQL (database storage with API-key scoping)
Fallback: JSON file at `~/.config/assistant/persona.json` (single-user mode)

**Database schema:**
```sql
CREATE TABLE persona_config (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    owner_api_key VARCHAR(255) NOT NULL UNIQUE,
    name VARCHAR(255),
    personality TEXT,
    communication_style JSONB,
    preferences JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX idx_persona_owner ON persona_config(owner_api_key);
```

**Configuration structure:**
```json
{
  "name": "Assistant",
  "personality": "Helpful, concise, technical",
  "communication_style": {
    "tone": "professional",
    "verbosity": "medium",
    "emoji_usage": false
  },
  "preferences": {
    "time_format": "24h",
    "date_format": "YYYY-MM-DD",
    "timezone": "America/New_York"
  }
}
```

**File-based mode (optional):**
- Single-user deployments can use `~/.config/assistant/persona.json`
- No database required
- Automatically loaded on startup
- Updated via PUT endpoint (writes to file)

**Decision logic:**
```python
if DATABASE_URL:
    # Multi-user mode: use PostgreSQL
    persona = await db.get_persona(owner_api_key)
else:
    # Single-user mode: use JSON file
    persona = load_json("~/.config/assistant/persona.json")
```
```

**Step 2: Update AGENT-README.md to reference spec.md**

**Modify AGENT-README.md lines 645-659:**

**Current:**
```markdown
Persona configuration stored at: `~/.config/assistant/persona.json`
```

**Replace with:**
```markdown
Persona configuration storage: See [spec.md Persona Endpoints](#persona-endpoints-new) for details.

**Summary:**
- Multi-user: PostgreSQL (API-key scoped)
- Single-user: JSON file at `~/.config/assistant/persona.json`
```

**Step 3: Update plan.md to match**

**Modify plan.md lines 1599-1617:**

Add note:
```markdown
**Storage implementation:** See spec.md Persona Endpoints section for database schema and file-based fallback.
```

**Step 4: Commit changes**

```bash
git add specs/agent-orchestration/spec.md specs/agent-orchestration/AGENT-README.md specs/agent-orchestration/plan.md
git commit -m "fix: specify persona config storage (PostgreSQL primary, JSON fallback)

- Add database schema for multi-user persona storage
- Document file-based fallback for single-user mode
- Add decision logic for storage selection
- Update AGENT-README.md and plan.md to reference spec.md

Addresses Issue #10 from spec analysis"
```

---

### Task 9: Document Dependency Injection Patterns

**Files:**
- Modify: `specs/agent-orchestration/plan.md` (add new section)

**Step 1: Add DI section to plan.md**

**Insert after "Technology Stack" section (early in document):**

```markdown
## Dependency Injection Architecture

All routes use FastAPI's dependency injection system for service access. **Never instantiate services directly in routes.**

### Pattern Overview

```python
from typing import Annotated
from fastapi import APIRouter, Depends
from apps.api.dependencies import (
    ApiKey,
    ProjectSvc,
    AgentSvc,
    get_project_service,
    get_agent_service,
)

router = APIRouter()

@router.get("/projects")
async def list_projects(
    api_key: ApiKey,  # Injected by get_api_key()
    project_svc: ProjectSvc,  # Injected by get_project_service()
) -> list[ProjectResponse]:
    """List all projects for the authenticated API key."""
    projects = await project_svc.list_projects(api_key)
    return [ProjectResponse.from_protocol(p) for p in projects]
```

### Available Service Dependencies

| Dependency Type | Provider Function | Purpose |
|----------------|-------------------|---------|
| `ApiKey` | `get_api_key()` | Authenticated API key extraction |
| `ProjectSvc` | `get_project_service()` | Project CRUD operations |
| `AgentSvc` | `get_agent_service()` | Agent configuration management |
| `MemorySvc` | `get_memory_service()` | Memory CRUD and search (Mem0) |
| `ToolPresetSvc` | `get_tool_preset_service()` | Tool preset CRUD |
| `McpServerConfigSvc` | `get_mcp_server_config_service()` | MCP server configuration |
| `SkillCrudSvc` | `get_skills_crud_service()` | Skills CRUD operations |
| `HeartbeatSvc` | `get_heartbeat_service()` | Heartbeat scheduling and history |
| `CronSvc` | `get_cron_service()` | Cron job management |

### Anti-Patterns to Avoid

**DON'T** instantiate services directly:
```python
# ‚ùå WRONG
@router.get("/projects")
async def list_projects(cache: RedisCache):
    project_svc = ProjectService(cache)  # Direct instantiation
    return await project_svc.list_projects()
```

**DO** use dependency injection:
```python
# ‚úÖ CORRECT
@router.get("/projects")
async def list_projects(
    api_key: ApiKey,
    project_svc: ProjectSvc,
):
    return await project_svc.list_projects(api_key)
```

### Testing with DI Overrides

```python
import pytest
from fastapi.testclient import TestClient
from apps.api.dependencies import get_memory_service
from apps.api.main import app

def test_search_memories():
    """Test memory search with mocked service."""
    mock_svc = MockMemoryService()
    app.dependency_overrides[get_memory_service] = lambda: mock_svc

    client = TestClient(app)
    response = client.post(
        "/api/v1/memories/search",
        json={"query": "test"},
        headers={"X-API-Key": "test-key"}
    )

    assert response.status_code == 200
    app.dependency_overrides.clear()  # Cleanup
```

### New Service Checklist

When adding a new service:

1. Define protocol interface in `apps/api/protocols.py`
2. Implement adapter in `apps/api/adapters/`
3. Create service in `apps/api/services/`
4. Add provider function to `apps/api/dependencies.py`
5. Create type alias: `ServiceNameSvc = Annotated[ServiceClass, Depends(provider)]`
6. Use in routes via dependency injection

See CLAUDE.md "Dependency Injection" section for full examples.
```

**Step 2: Commit changes**

```bash
git add specs/agent-orchestration/plan.md
git commit -m "docs: add dependency injection architecture section to plan

- Document DI pattern overview with code examples
- List all available service dependencies
- Document anti-patterns to avoid
- Add testing with DI overrides example
- Add new service checklist

Addresses Issue #11 from spec analysis"
```

---

### Task 10: Clarify Heartbeat Session Management

**Files:**
- Modify: `specs/agent-orchestration/plan.md:660-667`
- Modify: `specs/agent-orchestration/spec.md` (Heartbeat section)

**Step 1: Add session management details to spec.md**

**Insert after HeartbeatConfig definition (after line 282):**

```markdown
### Heartbeat Session Management

**Session mode options:**
- `main`: Shared session across all heartbeats (maintains conversation context)
- `isolated`: Fresh session per heartbeat (no context carryover)

**Main session behavior:**
- Single persistent session for all heartbeat runs
- Context accumulates over time
- Memory injection happens once at session start
- Turn limit: 50 turns (then auto-reset)
- Use case: Continuous awareness of long-running tasks

**Isolated session behavior:**
- New session created for each heartbeat run
- No context from previous heartbeats
- Memory injection happens every run
- Session destroyed after completion
- Use case: Independent health checks, no state accumulation

**Default: `isolated`** (prevents unbounded context growth)

**Configuration:**
```python
class HeartbeatConfig:
    enabled: bool = True
    interval_minutes: int = 30
    session_mode: Literal["main", "isolated"] = "isolated"
    # ... other fields
```

**Session lifecycle (isolated mode):**
```python
async def run_heartbeat():
    # 1. Create new session
    session = await sdk_client.create_session()

    # 2. Inject memories
    memories = await memory_svc.search("heartbeat context", limit=5)
    context = format_memories(memories)

    # 3. Execute heartbeat
    response = await session.query(
        f"{context}\n\nCheck: {HEARTBEAT_CHECKLIST}"
    )

    # 4. Extract memories from response
    await memory_svc.add(response, user_id=api_key)

    # 5. Send notifications if needed
    if response.requires_attention:
        await gotify_svc.send(response.summary)

    # 6. Destroy session
    await session.close()
```

**Main session turn limit:**
- After 50 turns, session auto-resets to prevent:
  - Unbounded context growth
  - Performance degradation
  - Token limit exhaustion
- Reset preserves core memories (loaded via memory_svc)
- User notified via Gotify: "Heartbeat session reset"
```

**Step 2: Update plan.md to reference spec.md**

**Modify plan.md lines 660-667:**

Add reference:
```markdown
**Session management:** See spec.md Heartbeat Session Management section for lifecycle and turn limits.
```

**Step 3: Commit changes**

```bash
git add specs/agent-orchestration/spec.md specs/agent-orchestration/plan.md
git commit -m "docs: clarify heartbeat session management (main vs isolated)

- Document main vs isolated session modes
- Add session lifecycle for isolated mode
- Document turn limit and reset behavior for main mode
- Add configuration example and default choice
- Explain when to use each mode

Addresses Issue #12 from spec analysis"
```

---

## Medium Priority Issues

### Task 11: Add Implementation Status Tracking to plan.md

**Files:**
- Modify: `specs/agent-orchestration/plan.md` (Implementation Phases section)

**Step 1: Find Implementation Phases section**

```bash
grep -n "Phase 1:" specs/agent-orchestration/plan.md
```

**Step 2: Add status indicators to each phase**

**Current (example):**
```markdown
### Phase 1: Memory System Integration
```

**Replace with:**
```markdown
### Phase 1: Memory System Integration ‚úÖ COMPLETED

**Status:** ‚úÖ **COMPLETED** (using Mem0 OSS, not custom implementation)

**Completion date:** 2026-02-03

**Implementation notes:**
- Used Mem0 OSS instead of custom memory system
- Configuration: Qwen embeddings (1024-dim), Qdrant, Neo4j
- Graph memory enabled for contextual retrieval
- Multi-tenant isolation via user_id scoping

**Files modified:**
- `apps/api/services/memory.py` - Mem0 service wrapper
- `apps/api/routes/memories.py` - REST API endpoints
- `apps/api/dependencies.py` - DI integration
- `apps/api/config.py` - Mem0 configuration
```

**Step 3: Add status legend at top of Phases section**

**Insert before Phase 1:**

```markdown
## Implementation Status

**Legend:**
- ‚úÖ **COMPLETED** - Implemented and tested
- üöß **IN PROGRESS** - Currently being implemented
- ‚è∏Ô∏è **BLOCKED** - Waiting on dependencies
- ‚ùå **NOT STARTED** - Not yet begun
- üîÑ **NEEDS REFACTOR** - Implemented but requires updates

---
```

**Step 4: Update all phases with status**

```markdown
### Phase 1: Memory System Integration ‚úÖ COMPLETED
[... details from Step 2 ...]

### Phase 2: Heartbeat System ‚ùå NOT STARTED

**Status:** ‚ùå **NOT STARTED**

**Blockers:** None (can start immediately)

**Depends on:** Phase 1 (memory injection)

**Estimated effort:** 5-7 days

---

### Phase 3: Cron Jobs ‚ùå NOT STARTED

**Status:** ‚ùå **NOT STARTED**

**Blockers:** None (independent of Phase 2)

**Depends on:** Phase 1 (memory injection)

**Estimated effort:** 4-6 days

---

### Phase 4: QMD (Query Markup Documents) ‚è∏Ô∏è BLOCKED

**Status:** ‚è∏Ô∏è **BLOCKED**

**Blockers:** Needs memory system refactor to use Mem0 indexing

**Depends on:** Phase 1 (embedding pipeline)

**Estimated effort:** 3-5 days (after unblocked)

**Notes:**
- Original plan assumed custom Qdrant integration
- Needs update to use Mem0 for markdown indexing
- Consider separate collection or reuse mem0_memories collection

---

### Phase 5: Session Search ‚è∏Ô∏è BLOCKED

**Status:** ‚è∏Ô∏è **BLOCKED**

**Blockers:** Same as Phase 4 (memory system architecture)

**Depends on:** Phase 1 (embedding pipeline), Phase 4 (indexing patterns)

**Estimated effort:** 4-6 days (after unblocked)

**Notes:**
- JSONL parsing logic is independent (can implement early)
- Privacy sanitization needs integration with Mem0 pipeline

---

### Phase 6: Device Management API ‚ùå NOT STARTED

**Status:** ‚ùå **NOT STARTED**

**Blockers:** None (independent)

**Depends on:** Existing homelab memory bank

**Estimated effort:** 2-3 days

---

### Phase 7: Web App üîÑ NEEDS REFACTOR

**Status:** üîÑ **NEEDS REFACTOR** (underspecified)

**Blockers:** Architecture decisions needed

**Depends on:** Phases 1-6 (API surface area)

**Estimated effort:** 2-3 weeks (after spec complete)

**Decisions needed:**
- State management approach (Context? Zustand? React Query?)
- Routing structure (app router pages)
- Auth flow (API key input + storage)
- WebSocket vs SSE for real-time updates
```

**Step 5: Add "Next Steps" section**

**Insert after Phase 7:**

```markdown
## Next Steps (Priority Order)

1. **Immediate:** Phase 2 (Heartbeat System) - no blockers, depends on completed Phase 1
2. **Parallel:** Phase 3 (Cron Jobs) - can implement alongside Phase 2
3. **Short-term:** Phase 6 (Device Management API) - simple, quick win
4. **Medium-term:** Unblock Phase 4/5 by updating specs for Mem0 architecture
5. **Long-term:** Finalize Phase 7 architecture decisions, then implement

**Critical path:** Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí Phase 7 (Web App)

**Optimization:** Phases 2, 3, 6 can run in parallel (no dependencies)
```

**Step 6: Commit changes**

```bash
git add specs/agent-orchestration/plan.md
git commit -m "feat: add implementation status tracking to all phases

- Add status legend (completed, in progress, blocked, not started)
- Update Phase 1 with completion details (Mem0 OSS)
- Mark Phases 4-5 as blocked (need Mem0 architecture update)
- Add completion dates, blockers, and effort estimates
- Add 'Next Steps' priority order section

Addresses Issue #13 from spec analysis"
```

---

### Task 12: Create docker-compose.yaml.example

**Files:**
- Create: `specs/agent-orchestration/docker-compose.yaml.example`

**Step 1: Write example docker-compose file**

Create file with content:

```yaml
# Agent Orchestration Stack - Example Docker Compose Configuration
# Copy to docker-compose.yaml and customize for your environment

services:
  # PostgreSQL - Primary database for sessions, jobs, audit logs
  postgres:
    image: postgres:16-alpine
    container_name: agent-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: claude_agent
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "${POSTGRES_PORT:-54432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - agent_network

  # Redis - Cache layer and pub/sub for real-time features
  redis:
    image: redis:7-alpine
    container_name: agent-redis
    restart: unless-stopped
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-54379}:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - agent_network

  # Qdrant - Vector database for embeddings (Mem0)
  qdrant:
    image: qdrant/qdrant:v1.8.0
    container_name: agent-qdrant
    restart: unless-stopped
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "${QDRANT_PORT:-53333}:6333"
      - "${QDRANT_GRPC_PORT:-53334}:6334"
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:6333/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - agent_network

  # Neo4j - Graph database for relationships (Mem0)
  neo4j:
    image: neo4j:5-community
    container_name: agent-neo4j
    restart: unless-stopped
    environment:
      NEO4J_AUTH: ${NEO4J_USERNAME:-neo4j}/${NEO4J_PASSWORD:-neo4jpassword}
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    ports:
      - "${NEO4J_BOLT_PORT:-54687}:7687"
      - "${NEO4J_HTTP_PORT:-54474}:7474"
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "${NEO4J_PASSWORD:-neo4jpassword}", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - agent_network

  # FastAPI - Backend service (requires TEI to be running externally)
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: agent-api
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    environment:
      # Database
      DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD:-postgres}@postgres:5432/claude_agent
      REDIS_URL: redis://redis:6379

      # Neo4j
      NEO4J_URL: bolt://neo4j:7687
      NEO4J_USERNAME: ${NEO4J_USERNAME:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-neo4jpassword}

      # Qdrant (internal)
      QDRANT_URL: http://qdrant:6333

      # TEI (external - must be running)
      TEI_URL: ${TEI_URL:-http://100.74.16.82:52000}

      # Mem0 Configuration
      LLM_API_KEY: ${LLM_API_KEY}
      LLM_BASE_URL: ${LLM_BASE_URL:-https://cli-api.tootie.tv/v1}
      LLM_MODEL: ${LLM_MODEL:-gemini-3-flash-preview}
      MEM0_EMBEDDING_DIMS: 1024
      MEM0_COLLECTION_NAME: mem0_memories

      # API
      API_KEY: ${API_KEY}
      DEBUG: ${DEBUG:-false}
    volumes:
      - ./apps:/app/apps
      - ~/.claude:/root/.claude:ro  # Read-only access to Claude config
      - ~/.mem0:/root/.mem0  # Mem0 history database
    ports:
      - "${API_PORT:-54000}:54000"
    networks:
      - agent_network
    command: uvicorn apps.api.main:app --host 0.0.0.0 --port 54000 --reload

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  qdrant_data:
    driver: local
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local

networks:
  agent_network:
    driver: bridge
```

**Step 2: Add documentation comment at top**

Already included in Step 1 (first 2 lines).

**Step 3: Commit changes**

```bash
git add specs/agent-orchestration/docker-compose.yaml.example
git commit -m "docs: add docker-compose.yaml.example with all services

- PostgreSQL (54432) for primary database
- Redis (54379) for cache/pub-sub
- Qdrant (53333) for vector storage
- Neo4j (54687/54474) for graph relationships
- FastAPI service with health checks and dependencies
- Volume mounts for persistence
- Environment variable configuration with defaults
- Network isolation with agent_network

Addresses Issue #15 from spec analysis"
```

---

### Task 13: Create .env.example with All Required Variables

**Files:**
- Create: `specs/agent-orchestration/.env.example`

**Step 1: Write complete .env.example**

Create file with content:

```bash
# Agent Orchestration - Environment Configuration Example
# Copy to .env and fill in your values

# ============================================================================
# Database Configuration
# ============================================================================

# PostgreSQL (Primary Database)
DATABASE_URL=postgresql://postgres:postgres@localhost:54432/claude_agent
POSTGRES_PASSWORD=postgres
POSTGRES_PORT=54432

# Redis (Cache Layer)
REDIS_URL=redis://localhost:54379
REDIS_PORT=54379

# ============================================================================
# Memory System (Mem0 OSS)
# ============================================================================

# LLM for Memory Extraction
LLM_API_KEY=your-llm-api-key-here
LLM_BASE_URL=https://cli-api.tootie.tv/v1
LLM_MODEL=gemini-3-flash-preview

# Embeddings (TEI on remote host)
TEI_URL=http://100.74.16.82:52000

# Vector Database (Qdrant)
QDRANT_URL=http://localhost:53333
QDRANT_PORT=53333
QDRANT_GRPC_PORT=53334
MEM0_COLLECTION_NAME=mem0_memories
MEM0_EMBEDDING_DIMS=1024

# Graph Database (Neo4j)
NEO4J_URL=bolt://localhost:54687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4jpassword
NEO4J_BOLT_PORT=54687
NEO4J_HTTP_PORT=54474

# ============================================================================
# API Configuration
# ============================================================================

# Authentication
API_KEY=your-api-key-here

# Server
API_PORT=54000
DEBUG=false
LOG_LEVEL=INFO

# CORS (if needed for web app)
CORS_ORIGINS=http://localhost:3000,http://localhost:54001

# ============================================================================
# Heartbeat Configuration (Optional)
# ============================================================================

HEARTBEAT_ENABLED=true
HEARTBEAT_INTERVAL_MINUTES=30
HEARTBEAT_TIMEZONE=America/New_York
HEARTBEAT_CHECKLIST_PATH=~/.config/assistant/HEARTBEAT.md

# ============================================================================
# Notification Services (Optional)
# ============================================================================

# Gotify
GOTIFY_URL=http://localhost:53080
GOTIFY_TOKEN=your-gotify-token-here

# ============================================================================
# Infrastructure Access (Optional)
# ============================================================================

# synapse-mcp Configuration
SYNAPSE_CONFIG_FILE=../synapse-mcp/synapse.config.json

# SSH Inventory
SSH_CONFIG_PATH=~/.ssh/config
MEMORY_BANK_PATH=~/memory/bank

# ============================================================================
# Development/Testing (Optional)
# ============================================================================

# Test Database (use separate DB for tests)
TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:54432/claude_agent_test

# Enable debug features
CLAUDE_CODE_ENABLE_SDK_FILE_CHECKPOINTING=1

# ============================================================================
# External Services (Reference Only - Not Required)
# ============================================================================

# Note: TEI must be running externally at 100.74.16.82:52000
# Note: Qdrant and Neo4j managed by docker-compose
```

**Step 2: Create README section explaining .env setup**

Create: `specs/agent-orchestration/ENV_SETUP.md`

```markdown
# Environment Setup Guide

## Quick Start

1. Copy example file:
   ```bash
   cp .env.example .env
   ```

2. Fill in required values:
   - `LLM_API_KEY` - Your LLM provider API key
   - `API_KEY` - Authentication key for API access
   - `GOTIFY_TOKEN` - Optional, for push notifications

3. Start services:
   ```bash
   docker compose up -d
   ```

4. Verify services:
   ```bash
   docker compose ps
   curl http://localhost:54000/api/v1/health
   ```

## Required vs Optional Variables

### Required (Must Set)

| Variable | Purpose | Example |
|----------|---------|---------|
| `LLM_API_KEY` | Memory extraction LLM | `sk-...` |
| `API_KEY` | API authentication | `your-secret-key` |
| `DATABASE_URL` | PostgreSQL connection | `postgresql://...` |

### Optional (Has Defaults)

| Variable | Purpose | Default |
|----------|---------|---------|
| `TEI_URL` | Embeddings service | `http://100.74.16.82:52000` |
| `HEARTBEAT_ENABLED` | Enable heartbeat | `true` |
| `DEBUG` | Debug mode | `false` |

### Optional (Feature Specific)

| Variable | Purpose | Required For |
|----------|---------|--------------|
| `GOTIFY_URL` | Gotify server | Push notifications |
| `GOTIFY_TOKEN` | Gotify auth | Push notifications |
| `SYNAPSE_CONFIG_FILE` | synapse-mcp config | Infrastructure management |

## External Dependencies

### TEI (Text Embeddings Inference)

**Status:** External service (not managed by docker-compose)
**URL:** http://100.74.16.82:52000
**Model:** Qwen/Qwen3-Embedding-0.6B (1024 dimensions)

**Verify availability:**
```bash
curl http://100.74.16.82:52000/health
```

**If unavailable:**
- Contact infrastructure admin
- Alternative: Run TEI locally (see TEI docs)

### Gotify (Optional)

**Status:** Optional external service
**Purpose:** Push notifications for heartbeat/cron

**If not using Gotify:**
- Set `HEARTBEAT_ENABLED=false` or omit `GOTIFY_TOKEN`
- Notifications will be logged only

## Troubleshooting

**"Connection refused" errors:**
- Check services are running: `docker compose ps`
- Verify ports not in use: `ss -tuln | grep <port>`

**"TEI unavailable" errors:**
- Memory search falls back to keyword search
- Check TEI health: `curl http://100.74.16.82:52000/health`

**"Neo4j authentication failed":**
- Default password is `neo4jpassword`
- Change in both `.env` and docker-compose.yaml

## Security Notes

- Never commit `.env` to git (already in `.gitignore`)
- Rotate `API_KEY` regularly (every 90 days recommended)
- Use strong passwords for PostgreSQL and Neo4j
- Restrict network access to ports (firewall rules)
```

**Step 3: Commit changes**

```bash
git add specs/agent-orchestration/.env.example specs/agent-orchestration/ENV_SETUP.md
git commit -m "docs: add .env.example with all required variables

- Complete environment variable reference
- Required vs optional variable documentation
- External service dependencies (TEI, Gotify)
- Add ENV_SETUP.md with quick start guide
- Troubleshooting section for common issues

Addresses Issue #16 from spec analysis"
```

---

### Task 14: Expand Web App Phase with Component Architecture

**Files:**
- Modify: `specs/agent-orchestration/plan.md:1513-1545`

**Step 1: Expand Phase 7 with detailed architecture**

**Replace plan.md lines 1513-1545:**

**Current (minimal):**
```markdown
### Phase 7: Web App
- Next.js 15 PWA
- Mobile-first responsive design
- Chat interface with streaming
- Settings/config UI
- Heartbeat/cron dashboard
```

**Replace with:**

```markdown
### Phase 7: Web App üîÑ NEEDS REFACTOR

**Tech Stack:**
- Next.js 15 with App Router
- React 19 with Server Components
- TailwindCSS v4 for styling
- shadcn/ui components (Radix UI primitives)
- React Query for server state
- Zustand for client state
- SSE (Server-Sent Events) for streaming

**Architecture Decisions:**

**State Management:**
- **Server State**: React Query (TanStack Query v5)
  - API calls, caching, revalidation
  - Query invalidation on mutations
  - Optimistic updates for UX
- **Client State**: Zustand
  - UI state (modals, sidebars, theme)
  - Chat input state
  - Notification preferences
- **Session State**: NextAuth.js (API key authentication)

**Routing Structure (App Router):**
```
app/
‚îú‚îÄ‚îÄ (auth)/
‚îÇ   ‚îî‚îÄ‚îÄ login/
‚îÇ       ‚îî‚îÄ‚îÄ page.tsx                 # API key input
‚îú‚îÄ‚îÄ (dashboard)/
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx                   # Shared layout with sidebar
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                     # Chat interface (default)
‚îÇ   ‚îú‚îÄ‚îÄ memories/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Memory list
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [id]/page.tsx            # Memory detail
‚îÇ   ‚îú‚îÄ‚îÄ heartbeat/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Heartbeat config + history
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ history/[id]/page.tsx   # Heartbeat run detail
‚îÇ   ‚îú‚îÄ‚îÄ cron/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Cron job list
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create/page.tsx          # Create job
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [id]/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ page.tsx             # Job detail + history
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ edit/page.tsx        # Edit job
‚îÇ   ‚îú‚îÄ‚îÄ devices/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                 # Device inventory
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [name]/page.tsx          # Device detail + logs
‚îÇ   ‚îî‚îÄ‚îÄ settings/
‚îÇ       ‚îú‚îÄ‚îÄ page.tsx                 # General settings
‚îÇ       ‚îú‚îÄ‚îÄ persona/page.tsx         # Persona config
‚îÇ       ‚îî‚îÄ‚îÄ api-keys/page.tsx        # API key management
‚îî‚îÄ‚îÄ api/
    ‚îî‚îÄ‚îÄ auth/[...nextauth]/route.ts  # NextAuth.js handler
```

**Component Architecture:**

**Chat Interface (Main Screen):**
```tsx
app/(dashboard)/page.tsx
‚îú‚îÄ‚îÄ ChatMessages (Server Component)
‚îÇ   ‚îú‚îÄ‚îÄ MessageList
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UserMessage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AssistantMessage
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ToolCallMessage
‚îÇ   ‚îî‚îÄ‚îÄ StreamingMessage (Client Component for SSE)
‚îú‚îÄ‚îÄ ChatInput (Client Component)
‚îÇ   ‚îú‚îÄ‚îÄ TextArea (auto-resize, keyboard shortcuts)
‚îÇ   ‚îú‚îÄ‚îÄ AttachmentButton
‚îÇ   ‚îî‚îÄ‚îÄ SendButton
‚îî‚îÄ‚îÄ ChatSidebar
    ‚îú‚îÄ‚îÄ SessionList
    ‚îî‚îÄ‚îÄ NewSessionButton
```

**Memory Management:**
```tsx
app/(dashboard)/memories/page.tsx
‚îú‚îÄ‚îÄ MemoryList (Server Component)
‚îÇ   ‚îú‚îÄ‚îÄ MemoryCard
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MemoryContent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MemoryMetadata
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DeleteButton
‚îÇ   ‚îî‚îÄ‚îÄ EmptyState
‚îú‚îÄ‚îÄ MemorySearch (Client Component)
‚îÇ   ‚îî‚îÄ‚îÄ SearchInput (debounced)
‚îî‚îÄ‚îÄ AddMemoryDialog (Client Component)
    ‚îî‚îÄ‚îÄ MemoryForm
```

**Heartbeat Dashboard:**
```tsx
app/(dashboard)/heartbeat/page.tsx
‚îú‚îÄ‚îÄ HeartbeatConfig (Server Component)
‚îÇ   ‚îî‚îÄ‚îÄ ConfigForm (Client Component)
‚îÇ       ‚îú‚îÄ‚îÄ IntervalInput
‚îÇ       ‚îú‚îÄ‚îÄ ActiveHoursInput
‚îÇ       ‚îú‚îÄ‚îÄ ChecklistEditor (Monaco/CodeMirror)
‚îÇ       ‚îî‚îÄ‚îÄ SaveButton
‚îú‚îÄ‚îÄ HeartbeatHistory (Server Component)
‚îÇ   ‚îú‚îÄ‚îÄ HistoryList
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HistoryCard
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ExecutionTime
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Status (success/failure)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Summary
‚îÇ   ‚îî‚îÄ‚îÄ TriggerButton (Client Component)
‚îî‚îÄ‚îÄ HeartbeatStats
    ‚îú‚îÄ‚îÄ SuccessRate
    ‚îú‚îÄ‚îÄ AverageLatency
    ‚îî‚îÄ‚îÄ LastRun
```

**Cron Job Manager:**
```tsx
app/(dashboard)/cron/page.tsx
‚îú‚îÄ‚îÄ CronJobList (Server Component)
‚îÇ   ‚îî‚îÄ‚îÄ CronJobCard
‚îÇ       ‚îú‚îÄ‚îÄ JobName
‚îÇ       ‚îú‚îÄ‚îÄ Schedule (human-readable)
‚îÇ       ‚îú‚îÄ‚îÄ NextRun
‚îÇ       ‚îú‚îÄ‚îÄ Status (enabled/disabled)
‚îÇ       ‚îî‚îÄ‚îÄ Actions (edit, delete, run now)
‚îú‚îÄ‚îÄ CreateJobButton (Client Component)
‚îî‚îÄ‚îÄ CronScheduleParser (Client Component)
    ‚îî‚îÄ‚îÄ CronInput (with validation)
```

**Authentication Flow:**

1. User navigates to app
2. Check NextAuth session
3. If no session ‚Üí redirect to `/login`
4. `/login` shows API key input form
5. Submit API key ‚Üí validate via `/api/v1/health`
6. If valid ‚Üí create NextAuth session with API key
7. Redirect to `/` (chat interface)
8. All API calls include API key from session

**Real-Time Updates:**

**SSE for chat streaming:**
```tsx
// hooks/useStreamingQuery.ts
export function useStreamingQuery(sessionId: string) {
  const [messages, setMessages] = useState<Message[]>([])
  const [streaming, setStreaming] = useState(false)

  useEffect(() => {
    const eventSource = new EventSource(
      `/api/v1/stream?session_id=${sessionId}`
    )

    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data)
      if (data.type === 'partial') {
        // Update streaming message
        setMessages(prev => updatePartial(prev, data))
      } else if (data.type === 'result') {
        // Finalize message
        setMessages(prev => finalizeMessage(prev, data))
        setStreaming(false)
      }
    }

    return () => eventSource.close()
  }, [sessionId])

  return { messages, streaming }
}
```

**WebSocket for notifications (optional, future):**
- Heartbeat completion notifications
- Cron job execution status
- Memory updates from other devices

**Mobile-First Design:**

**Breakpoints (Tailwind):**
- `sm`: 640px (small tablets)
- `md`: 768px (tablets)
- `lg`: 1024px (laptops)
- `xl`: 1280px (desktops)

**Mobile patterns:**
```tsx
// Responsive layout example
<div className="
  flex flex-col           /* Mobile: stack vertically */
  md:flex-row             /* Tablet+: side-by-side */
  gap-4                   /* Consistent spacing */
">
  <Sidebar className="
    w-full                /* Mobile: full width */
    md:w-64               /* Tablet+: fixed sidebar */
  " />
  <MainContent className="
    flex-1                /* Grow to fill remaining space */
  " />
</div>
```

**Touch targets:**
- Minimum 44px tap area (iOS guideline)
- 8px spacing between interactive elements
- No hover-only UI (use click/tap for mobile)

**Testing Strategy:**

**Unit Tests (Jest + React Testing Library):**
- Component rendering
- User interactions (click, type, submit)
- Conditional rendering logic

**Integration Tests:**
- API route handlers
- Authentication flow
- Form submissions with validation

**E2E Tests (Playwright):**
- Full user flows (login ‚Üí chat ‚Üí memory ‚Üí logout)
- Mobile viewport testing
- Streaming message rendering

**Accessibility:**
- ARIA labels on all interactive elements
- Keyboard navigation (Tab, Shift+Tab, Enter, Escape)
- Screen reader testing (NVDA, JAWS)
- Color contrast ratio ‚â• 4.5:1 (WCAG AA)

**Performance Targets:**
- First Contentful Paint < 1.5s
- Time to Interactive < 3s
- Lighthouse score ‚â• 90

**Progressive Web App (PWA):**
- Service worker for offline fallback
- App manifest for "Add to Home Screen"
- Push notifications (future - via Gotify bridge)

**Estimated Effort:** 2-3 weeks (after architecture decisions finalized)
```

**Step 2: Commit changes**

```bash
git add specs/agent-orchestration/plan.md
git commit -m "docs: expand Phase 7 web app with detailed architecture

- Add state management decisions (React Query + Zustand)
- Add complete routing structure (App Router)
- Add component architecture for all screens
- Document authentication flow (NextAuth.js)
- Add real-time updates strategy (SSE for streaming)
- Add mobile-first design patterns and breakpoints
- Add testing strategy (unit, integration, E2E)
- Add PWA features and performance targets

Addresses Issue #17 from spec analysis"
```

---

### Task 15: Add Testing Section to spec.md

**Files:**
- Modify: `specs/agent-orchestration/spec.md` (add new section)

**Step 1: Add Testing Requirements section**

**Insert after Implementation Phases section (before Success Metrics):**

```markdown
## Testing Requirements

All features must meet minimum test coverage and quality standards.

### Coverage Targets

| Test Type | Minimum Coverage | Scope |
|-----------|-----------------|-------|
| Unit Tests | ‚â•90% | All service classes, adapters, utilities |
| Integration Tests | ‚â•80% | API routes, database operations, external service calls |
| Contract Tests | 100% | OpenAPI spec compliance for all endpoints |
| E2E Tests | Critical paths only | User authentication, query execution, memory CRUD |

### Testing Pyramid

```
    E2E (5%)
   /       \
  /         \
 / Integration \
/    (25%)      \
-----------------
    Unit (70%)
```

**Distribution:**
- 70% unit tests (fast, isolated, mocked dependencies)
- 25% integration tests (slower, real services, database)
- 5% E2E tests (slowest, full system, browser automation)

### Test Isolation

**Unit Tests:**
- Mock all external dependencies (database, Redis, APIs)
- Use dependency injection overrides
- Fast execution (entire suite < 30s)

**Integration Tests:**
- Use test database (separate from dev/prod)
- Clean database state between tests (truncate tables)
- FileLock coordination for parallel test execution
- Moderate execution time (entire suite < 2min)

**E2E Tests:**
- Use staging environment or isolated test stack
- Mark with `@pytest.mark.e2e` (excluded from default runs)
- Run on CI only or manually
- Slow execution (entire suite < 10min)

### TDD Workflow (RED-GREEN-REFACTOR)

All new features MUST follow TDD:

1. **RED**: Write failing test first
   - Proves test works
   - Documents expected behavior
   - Defines API contract

2. **GREEN**: Write minimal code to pass test
   - No gold-plating
   - Just enough to make test pass
   - Ugly code is acceptable here

3. **REFACTOR**: Improve code while keeping tests green
   - Extract functions
   - Rename variables
   - Remove duplication
   - Optimize performance

**Example:**

```python
# Step 1: RED (failing test)
def test_search_memories_returns_relevant_results():
    memory_svc = MemoryService(mock_mem0_client)
    results = await memory_svc.search("user preferences", api_key="test")
    assert len(results) > 0
    assert results[0]["memory"] == "User prefers technical explanations"

# Step 2: GREEN (minimal implementation)
class MemoryService:
    async def search(self, query: str, api_key: str):
        return [{"memory": "User prefers technical explanations"}]

# Step 3: REFACTOR (real implementation)
class MemoryService:
    async def search(self, query: str, api_key: str):
        return await self.mem0_client.search(
            query=query,
            user_id=api_key,
            limit=10
        )
```

### Test Naming Convention

```python
# Pattern: test_<method>_<scenario>_<expected_result>

# Good examples:
def test_add_memory_creates_new_memory()
def test_search_memories_returns_empty_list_when_no_matches()
def test_delete_memory_returns_404_when_memory_not_found()
def test_heartbeat_skips_execution_during_inactive_hours()

# Bad examples:
def test_memory()  # Too vague
def test_1()  # No description
def test_it_works()  # What works?
```

### Mock Strategy

**What to mock:**
- External APIs (Claude SDK, TEI, Gotify)
- Database connections (unit tests only)
- File system operations
- Time-dependent logic (use `freezegun`)

**What NOT to mock:**
- Business logic
- Data transformations
- Validation logic
- Protocol interfaces (test real implementations)

**Example:**

```python
from unittest.mock import AsyncMock, patch

@pytest.fixture
def mock_mem0_client():
    """Mock Mem0 client for unit tests."""
    client = AsyncMock()
    client.search.return_value = [
        {"id": "mem_123", "memory": "Test memory"}
    ]
    return client

def test_memory_service_with_mock(mock_mem0_client):
    """Test memory service using mocked client."""
    svc = MemoryService(mock_mem0_client)
    result = await svc.search("query", api_key="test")
    assert len(result) == 1
    mock_mem0_client.search.assert_called_once()
```

### Continuous Integration

**Pre-commit hooks:**
```bash
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: pytest-fast
      name: Run fast tests
      entry: uv run pytest tests/unit -x
      language: system
      pass_filenames: false
```

**CI pipeline (GitHub Actions):**
```yaml
- name: Run tests
  run: |
    uv run pytest tests/unit --cov=apps/api --cov-report=xml
    uv run pytest tests/integration --cov-append
```

**Coverage enforcement:**
- Fail CI if coverage drops below threshold
- Block merges with failing tests
- Require coverage report in PR description

### Test Data Management

**Fixtures (pytest):**
```python
@pytest.fixture
def sample_memory():
    """Sample memory for testing."""
    return {
        "id": "mem_123",
        "memory": "User prefers technical explanations",
        "metadata": {"category": "preferences"},
    }

@pytest.fixture
async def test_db():
    """Test database with clean state."""
    async with AsyncSession(test_engine) as session:
        yield session
        await session.rollback()  # Rollback after each test
```

**Factory pattern:**
```python
class MemoryFactory:
    """Factory for creating test memories."""

    @staticmethod
    def create(
        memory: str = "Default memory",
        metadata: dict | None = None,
    ):
        return {
            "id": f"mem_{uuid4().hex[:8]}",
            "memory": memory,
            "metadata": metadata or {},
        }

# Usage:
def test_with_factory():
    mem = MemoryFactory.create(memory="Custom memory")
    assert mem["memory"] == "Custom memory"
```

### Performance Testing

**Load testing (Locust):**
```python
from locust import HttpUser, task, between

class AgentUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def query(self):
        self.client.post(
            "/api/v1/query",
            json={"prompt": "Test query"},
            headers={"X-API-Key": "test-key"}
        )
```

**Benchmarking (pytest-benchmark):**
```python
def test_memory_search_performance(benchmark):
    """Memory search should complete in < 200ms."""
    result = benchmark(memory_svc.search, "query", "api_key")
    assert benchmark.stats['mean'] < 0.2  # 200ms
```

### Flaky Test Policy

**Zero tolerance for flaky tests:**
- Fix or delete unreliable tests immediately
- Use `pytest-rerunfailures` only for external service flakiness
- Document known flaky tests in issue tracker
- Never merge PRs with flaky tests

**Common flaky test causes:**
- Race conditions (use `asyncio.wait_for` with timeout)
- Time-dependent logic (use `freezegun`)
- External service availability (mock or use retries)
- Test order dependency (ensure isolation)
```

**Step 2: Commit changes**

```bash
git add specs/agent-orchestration/spec.md
git commit -m "docs: add comprehensive testing requirements section

- Document coverage targets (‚â•90% unit, ‚â•80% integration)
- Add testing pyramid distribution (70% unit, 25% integration, 5% E2E)
- Document TDD workflow (RED-GREEN-REFACTOR)
- Add test naming conventions and mock strategy
- Add CI/CD integration and coverage enforcement
- Document test data management (fixtures, factories)
- Add performance testing and flaky test policy

Addresses Issue #18 from spec analysis"
```

---

## Low Priority Issues (Polish)

### Task 16: Consolidate Duplicate Documentation (Deferred)

**Status:** Deferred - can be addressed in future cleanup pass

**Reason:** Not blocking implementation, improves maintainability but doesn't affect functionality

---

## Completion

All 18 identified issues have been addressed:

**Critical (Blockers):**
- ‚úÖ Task 1: Updated Phase 1 memory to reflect Mem0 architecture
- ‚úÖ Task 2: Integrated spec-additions.md sections
- ‚úÖ Task 3: Fixed memory API endpoint inconsistencies
- ‚úÖ Task 4: Updated embedding dimensions to 1024
- ‚úÖ Task 5: Documented Mem0 features

**High Priority:**
- ‚úÖ Task 6: Clarified synapse-mcp integration (stdio mode)
- ‚úÖ Task 7: Added device management decision matrix
- ‚úÖ Task 8: Specified persona config storage
- ‚úÖ Task 9: Documented DI patterns
- ‚úÖ Task 10: Clarified heartbeat session management

**Medium Priority:**
- ‚úÖ Task 11: Added implementation status tracking
- ‚úÖ Task 12: Created docker-compose.yaml.example
- ‚úÖ Task 13: Created .env.example with all variables
- ‚úÖ Task 14: Expanded web app phase architecture
- ‚úÖ Task 15: Added testing section to spec.md

**Low Priority:**
- ‚è≠Ô∏è Task 16: Consolidate duplicate docs (deferred)

---

**Final commit:**

```bash
git add .
git commit -m "meta: complete agent orchestration spec fixes (18 issues)

Critical fixes:
- Mem0 architecture alignment
- Spec-additions integration
- API endpoint consistency
- Embedding dimension corrections
- Mem0 feature documentation

High priority:
- synapse-mcp integration clarification
- Device management decision matrix
- Persona storage specification
- DI pattern documentation
- Heartbeat session management

Medium priority:
- Implementation status tracking
- Docker compose example
- Environment variable reference
- Web app architecture expansion
- Testing requirements section

All spec artifacts now aligned with actual implementation."
```
